# vLLM + `speakleash/Bielik-11B-v2.3-Instruct-FP8` â€“ Przewodnik Szybkiego Startu (Ubuntu)

> **Wymagania wstÄ™pne**
> - Ubuntu 20.04 lub nowszy
> - Python 3.10 (w projekcie uÅ¼ywamy 3.10.6)
> - `virtualenv` (zainstalowany)
> - CUDA 11.8 + GPU **lub** Å›rodowisko tylko CPU

---  

## 1ï¸âƒ£ UtwÃ³rz i aktywuj wirtualne Å›rodowisko

```
mkdir -p ~/bielik && cd ~/bielik
python3 -m venv .venv
source .venv/bin/activate
```

> Powoduje to utworzenie katalogu projektu, przygotowanie wirtualnego Å›rodowiska w folderze `.venv` oraz jego
> aktywacjÄ™ (w promptcie pojawi siÄ™ `(.venv)`).

---  

## 2ï¸âƒ£ Zainstaluj **vLLM**

```
pip install --upgrade pip
pip install "vllm[cuda]"
```

> Instalacja najnowszej wersji **vLLM** z obsÅ‚ugÄ… GPU (CUDA zostanie wykryte automatycznie).  
> JeÅ›li nie masz GPU, uÅ¼yj wersji CPU: `pip install vllm[cpu]`.

---  

### SprawdÅº instalacjÄ™

```
python -c "import vllm; print(vllm.__version__)"
```

PowinieneÅ› zobaczyÄ‡ wersjÄ™, np. `0.11.2`.

---  

## 4ï¸âƒ£ Przygotuj Å›rodowisko do pobierania modelu

```
pip install huggingface_hub
```

---  

## 6ï¸âƒ£ Pobierz model `speakleash/Bielik-11B-v2.3-Instruct-FP8`

```
mkdir -p ./speakleash/Bielik-11B-v2.3-Instruct-FP8
hf download speakleash/Bielik-11B-v2.3-Instruct-FP8 \
    --local-dir ./speakleash/Bielik-11B-v2.3-Instruct-FP8
```

> Model zostanie pobrany do wskazanego katalogu. Pliki bÄ™dÄ… takÅ¼e buforowane domyÅ›lnie w `~/.cache/huggingface/hub`.

---  

### (Opcjonalnie) Ustaw wÅ‚asny katalog cache

JeÅ›li chcesz, aby wszystkie modele byÅ‚y przechowywane wewnÄ…trz projektu, ustaw zmiennÄ… przed pobraniem:

```
export HF_HOME=$PWD/.cache/huggingface   
# np. ./bielik/.cache/huggingface
```

---  

## 7ï¸âƒ£ Uruchom serwer **vLLM**

Skopiuj gotowy skrypt Bash (przykÅ‚adowa Å›cieÅ¼ka â€“ dostosuj do swojego projektu):

```
cp path/to/llm-router/examples/quickstart/speakleash-bielik-11b-v2_3-Instruct/run-bielik-11b-v2_3-vllm.sh .
bash run-bielik-11b-v2_3-vllm.sh
```

> **WskazÃ³wka:** uruchom serwer w sesji `tmux` lub `screen`, aby pozostawaÅ‚ aktywny po rozÅ‚Ä…czeniu siÄ™ z terminalem.

---  

## 8ï¸âƒ£ Przetestuj endpoint

> > **INFO**: `curl` i `jq` to narzÄ™dzia systemowe.


```
curl http://localhost:7000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "speakleash/Bielik-11B-v2.3-Instruct-FP8",
        "messages": [{"role": "user", "content": "CzeÅ›Ä‡, jak siÄ™ masz?"}],
        "max_tokens": 100
      }' | jq
```

PowinieneÅ› otrzymaÄ‡ odpowiedÅº w formacie JSON, np.:

```json
{
  "id": "chatcmpl-xxxx",
  "object": "chat.completion",
  "created": 1764516430,
  "model": "speakleash/Bielik-11B-v2.3-Instruct-FP8",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "CzeÅ›Ä‡! Jestem w peÅ‚ni sprawny i gotowy do rozmowy. Jak mogÄ™ Ci pomÃ³c?"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 15,
    "total_tokens": 66,
    "completion_tokens": 51
  }
}
```

---  

## 9ï¸âƒ£ Przydatne wskazÃ³wki

| Temat                       | Rekomendacja                                                                                                                              |
|-----------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| **PamiÄ™Ä‡**                  | `speakleash/Bielik-11B-v2.3-Instruct-FP8` potrzebuje ok. 12GB VRAM. UÅ¼yj `--cpu-offload` (jeÅ›li wspierane) przy ograniczonej pamiÄ™ci GPU. |
| **Lokalizacja cache**       | Ustaw `HF_HOME=$PWD/.cache/huggingface`, aby wszystkie pliki modelu znajdowaÅ‚y siÄ™ w katalogu projektu.                                   |
| **RÃ³wnolegÅ‚oÅ›Ä‡ tokenizera** | `export TOKENIZERS_PARALLELISM=false` wyciszy ostrzeÅ¼enia tokenizera.                                                                     |
| **WybÃ³r GPU**               | `export CUDA_VISIBLE_DEVICES=0` (lub inny indeks) przy wielu kartach GPU.                                                                 |
| **Aktualizacja**            | `pip install -U vllm` odÅ›wieÅ¼a bibliotekÄ™; przy nastÄ™pnym uruchomieniu serwera zostanÄ… pobrane nowsze pliki modelu, jeÅ›li sÄ… dostÄ™pne.    |
| **Dezaktywacja**            | Po zakoÅ„czeniu pracy wystarczy wpisaÄ‡ `deactivate`, aby opuÅ›ciÄ‡ wirtualne Å›rodowisko.                                                     |

---  

## ğŸ‰ Gotowe!

Masz juÅ¼ w peÅ‚ni dziaÅ‚ajÄ…ce API kompatybilne z OpenAI, oparte na **vLLM** i modelu
**speakleash/Bielik-11B-v2.3-Instruct-FP8**.