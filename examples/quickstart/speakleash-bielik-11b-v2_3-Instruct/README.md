# üöÄ **Przewodnik Szybkiego Startu** dla `speakleash/Bielik-11B-v2.3-Instruct` z **vLLM** & **LLM‚ÄëRouter**

Ten przewodnik prowadzi Ciƒô krok po kroku przez:

1. **Instalacjƒô vLLM** i modelu `speakleash/Bielik-11B-v2.3-Instruct`.
2. **Instalacjƒô LLM‚ÄëRouter** (bramki API).
3. **Uruchomienie routera** z konfiguracjƒÖ modeli dostarczonƒÖ w `models-config.json`.

Wszystkie polecenia zak≈ÇadajƒÖ, ≈ºe pracujesz na systemie Unix‚Äëlike (Linux/macOS) z **Python 3.10.6**, `virtualenv` oraz (
opcjonalnie) kartƒÖ GPU obs≈ÇugujƒÖcƒÖ CUDA 11.8.

---  

## üìã **Wymagania wstƒôpne**

| Wymaganie     | Szczeg√≥≈Çy                                                    |
|---------------|--------------------------------------------------------------|
| **OS**        | Ubuntu‚ÄØ20.04‚ÄØ+ (lub dowolna nowsza dystrybucja Linux/macOS)  |
| **Python**    | 3.10.6 (domy≈õlna wersja projektu)                            |
| **GPU**       | CUDA‚ÄØ11.8‚ÄØ+ (minimum 12‚ÄØGB VRAM) **lub** ≈õrodowisko CPU‚Äëonly |
| **Narzƒôdzia** | `git`, `curl`, `jq` (opcjonalnie, przydatne do testowania)   |
| **Sieƒá**      | Dostƒôp do PyPI oraz Hugging‚ÄØFace w celu pobrania modelu      |

---  

## 1Ô∏è‚É£ **Utworzenie i aktywacja wirtualnego ≈õrodowiska**

```shell script
# (opcjonalnie) utw√≥rz katalog demo i przejd≈∫ do niego
mkdir -p ~/bielik-demo && cd $_

# Inicjalizacja venv
python3 -m venv .venv
source .venv/bin/activate

# Aktualizacja pip (zawsze dobry pomys≈Ç)
pip install --upgrade pip
```

---  

## 2Ô∏è‚É£ Instalacja **vLLM** oraz pobranie modelu Bielik

> Pe≈ÇnƒÖ instrukcjƒô znajdziesz w pliku [`VLLM.md`](./VLLM.md).

---  

## 3Ô∏è‚É£ **Uruchomienie serwera vLLM**

Skopiuj do bie≈ºƒÖcego katalogu dostarczony skrypt Bash (dostosuj ≈õcie≈ºkƒô, je≈õli potrzebujesz) i uruchom go:

```shell script
cp path/to/llm-router/examples/quickstart/speakleash-bielik-11b-v2_3-Instruct/run-bielik-11b-v2_3-vllm.sh .
chmod +x run-bielik-11b-v2_3-vllm.sh

# Uruchom (warto w tmux/screen)
./run-bielik-11b-v2_3-vllm.sh
```

Serwer nas≈Çuchuje na **`http://0.0.0.0:7000`** i udostƒôpnia endpoint zgodny z OpenAI pod `/v1/chat/completions`.

Mo≈ºesz szybko go przetestowaƒá:

```shell script
curl http://localhost:7000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "speakleash/Bielik-11B-v2.3-Instruct",
        "messages": [{"role": "user", "content": "Cze≈õƒá, jak siƒô masz?"}],
        "max_tokens": 100
      }' | jq
```

Powiniene≈õ otrzymaƒá odpowied≈∫ w formacie JSON.

---  

## 4Ô∏è‚É£ Instalacja **LLM‚ÄëRouter**

```shell script
# Sklonuj repozytorium (je≈õli jeszcze go nie masz)
git clone https://github.com/radlab-dev-group/llm-router.git
cd llm-router

# Instalacja core + API (w tym samym venv)
pip install .[api]

# (Opcjonalnie) wsparcie dla Prometheus
pip install .[api,metrics]
```

> **Uwaga:** Router u≈ºywa tego samego wirtualnego ≈õrodowiska, kt√≥re utworzy≈Çe≈õ wcze≈õniej, wiƒôc wszystkie zale≈ºno≈õci
> pozostajƒÖ odizolowane.

---  

## 6Ô∏è‚É£ **Przygotowanie konfiguracji routera**

Plik `models-config.json` znajdujƒÖcy siƒô w katalogu **speakleash‚Äëbielik** ju≈º zawiera definicjƒô naszego modelu:

```json
{
  "speakleash_models": {
    "speakleash/Bielik-11B-v2.3-Instruct": {
      "providers": [
        {
          "id": "bielik-11B_v2_3-vllm-local:7000",
          "api_host": "http://localhost:7000/",
          "api_type": "vllm",
          "input_size": 56000,
          "weight": 1.0
        }
      ]
    }
  },
  "active_models": {
    "speakleash_models": [
      "speakleash/Bielik-11B-v2.3-Instruct"
    ]
  }
}
```

Skopiuj go (lub przenie≈õ) do katalogu `resources/configs/` routera:

```shell script
mkdir -p resources/configs
cp path/to/speakleash-bielik/models-config.json resources/configs/
```

---  

## 6Ô∏è‚É£ Uruchomienie **LLM‚ÄëRouter**

### Lokalny Gunicorn

W repozytorium znajduje siƒô pomocniczy skrypt `run-rest-api-gunicorn.sh`. Upewnij siƒô, ≈ºe jest wykonywalny, a nastƒôpnie
go uruchom:

```shell script
chmod +x run-rest-api-gunicorn.sh
./run-rest-api-gunicorn.sh
```

Domy≈õlne zmienne ≈õrodowiskowe (mo≈ºna zmieniƒá w skrypcie):

| Zmienna                     | Domy≈õlna warto≈õƒá                       | Opis                              |
|-----------------------------|----------------------------------------|-----------------------------------|
| `LLM_ROUTER_SERVER_TYPE`    | `gunicorn`                             | Backend serwera                   |
| `LLM_ROUTER_SERVER_PORT`    | `8080`                                 | Port nas≈Çuchiwania routera        |
| `LLM_ROUTER_MODELS_CONFIG`  | `resources/configs/models-config.json` | ≈öcie≈ºka do pliku konfiguracyjnego |
| `LLM_ROUTER_USE_PROMETHEUS` | `1` (je≈õli zainstalowano `metrics`)    | W≈ÇƒÖcza endpoint `/api/metrics`    |

Router bƒôdzie dostƒôpny pod **`http://0.0.0.0:8080/api`**.
Pe≈Çna lista dostƒôpnych zmiennych ≈õrodowiskowych znajduje siƒô w
[opisie zmiennych ≈õrodowiskowych](../../../llm_router_api/README.md#environment-variables)

---  

## 7Ô∏è‚É£ **Test pe≈Çnego stosu (router ‚Üí vLLM)**

```shell script
curl http://localhost:8080/api/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
        "model": "speakleash/Bielik-11B-v2.3-Instruct",
        "messages": [{"role": "user", "content": "Opowiedz kr√≥tki ≈ºart."}],
        "max_tokens": 80
      }' | jq
```

Zapytanie przechodzi przez **LLM‚ÄëRouter**, kt√≥ry przekazuje je do lokalnego serwera vLLM, a odpowied≈∫ zostaje zwr√≥cona w
formacie JSON.

---  

## üöÄ **Uruchamianie przyk≈Çad√≥w**

W folderze [`examples/`](../../../examples) znajduje siƒô szereg przyk≈Çad√≥w (LangChain, LlamaIndex, OpenAI SDK, LiteLLM,
Haystack). Aby je uruchomiƒá:

1. **Ustaw adres routera** ‚Äì w ≈õrodowisku (`LLM_ROUTER_HOST`) lub w pliku `examples/constants.py`:

```shell script
export LLM_ROUTER_HOST="http://localhost:8080/api"

# lub w Pythonie (constants.py):
HOST = "http://localhost:8080/api"
```

2. **Zainstaluj zale≈ºno≈õci przyk≈Çad√≥w**

```shell script
pip install -r examples/requirements.txt
```

3. **Uruchom wybrany przyk≈Çad**

```shell script
python examples/langchain_example.py
python examples/llamaindex_example.py
python examples/openai_example.py
python examples/litellm_example.py
python examples/haystack_example.py
```

Wszystkie pozosta≈Çe szczeg√≥≈Çy konfiguracji (obs≈Çuga prompt√≥w, strumieniowanie, u≈ºycie wielu modeli, obs≈Çuga b≈Çƒôd√≥w itp.)
sƒÖ udokumentowane w poszczeg√≥lnych plikach przyk≈Çad√≥w oraz w plikach [`examples/README.md`](../../README.md) i [
`examples/README_LLAMAINDEX.md`](../../README_LLAMAINDEX.md). Dostosuj jedynie zmiennƒÖ `HOST`/`LLM_ROUTER_HOST`
(ewentualnie tak≈ºe `MODELS`), a przyk≈Çady automatycznie odpytajƒÖ instancjƒô uruchomionego llm‚Äëroutera.

---  

## üéâ **Co dalej?**

| Obszar                    | Co mo≈ºesz zrobiƒá                                                                                                             |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------|
| **Prometheus**            | Je≈õli w≈ÇƒÖczy≈Çe≈õ `metrics`, dodaj endpoint `/api/metrics` do swojego systemu monitorujƒÖcego.                                  |
| **Guardrails & Masking**  | Ustaw zmienne `LLM_ROUTER_FORCE_MASKING`, `LLM_ROUTER_FORCE_GUARDRAIL_REQUEST`, itp., aby dodaƒá warstwy ochronne.            |
| **Wiele dostawc√≥w**       | Rozbuduj `models-config.json` o kolejne providery (np. Ollama, OpenAI) i eksperymentuj z r√≥≈ºnymi strategiami load‚Äëbalancing. |
| **Aktualizacje**          | `pip install -U vllm` oraz `pip install -U llm-router` zapewniƒÖ najnowsze poprawki i funkcje.                                |
| **Optymalizacja pamiƒôci** | Przy ograniczonej VRAM u≈ºyj flagi `--cpu-offload` w skrypcie `run-bielik-11b-v2_3-vllm.sh`.                                  |

---  

**Powodzenia i mi≈Çego korzystania z modelu Bielik!**

---