## Changelog

| Version | Changelog                                                                                                                                                                                                                                                                                   |
|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 0.0.1   | Initialization, License, setup, interface for each endpoint and sample `ping` EP. Autoloader of builtin endpoints and for the future implementations.                                                                                                                                       |
| 0.0.2   | Add base models for api call (module `llm_proxy_rest.data_models` with `error.py` handling. Decorators to check required params and to measure the response time.                                                                                                                           |
| 0.0.3   | Proper `AutoLoading` for each found endpoint. Implementation of `ApiTypesDispatcher`, `ApiModelConfig`, `ModelHandler`. Ollama endpoints: `/`, `tags`. Added endpoint to full proxy with params. Streaming in case when external api provides stream.                                       |
| 0.0.4   | All llama-service endpoints are refactored to `llm-proxy-api`. Refactoring base `ep_run` method. Proper handling system message, prompt name, model etc.                                                                                                                                    |
| 0.1.0   | Repository name changed from `llm-proxy-api` to `llm-router`. Added class `HttpRequestExecutor` to handle http requests from `EndpointWithHttpRequestI`. Handled routing between any models: `openai -> ollama` and `ollama -> openai`                                                      |
| 0.1.1   | Prometheus metrics logging. Workers/Threads/Workers class is able to set by environments. Streaming fixes. Multi-providers for single model with default-balanced strategy.                                                                                                                 |
| 0.2.0   | Add balancing strategies: `balanced`, `weighted`, `dynamic_weighted` and `first_available` which works for streaming and non streaming requests. Included Prometheus metrics logging via `/metrics` endpoint. First stage of `llm_router_lib` library, to simply usage of `llm-router-api`. |
| 0.2.1   | Fix stream: OpenAI->Ollama, Ollama->OpenAI. Add Redis caching of availability of model providers (when using `first_available` strategy). Add `llm_router_web` module with simple flask-based frontend to manage llm-router config files.                                                   |
| 0.2.2   | Update dockerfile and requirements. Fix routing with vLLM.                                                                                                                                                                                                                                  |
| 0.2.3   | New web configurator: Handling projects, configs for each user separately. First Available strategy is more powerful, a lot of improvements to efficiency.                                                                                                                                  |
| 0.2.4   | Anonymizer module, integration anonymization with any endpoint (using dynamic payload analysis and full payload anonymisation), dedicated `/api/anonymize_text` endpoint as memory only anonymization. Whole router may be run in `FORCE_ANONYMISATION` mode.                               |
| 0.3.0   | Anonymization available with three strategies: `fast_masker`, `genai`, `prov_masker`.                                                                                                                                                                                                       |
| 0.3.1   | Refactoring `lb.strategies` to be more flexible modular. Introduced `MaskerPipeline` and `GuardrailPipeline` both configured via env. Removed genai-based masking endpoint.                                                                                                                 |
| 0.4.0   | The main repository is divided into dedicated ones: plugins, services, web — separate repositories. Clean up the whole repository. Examples of integration with llamaindex, langchain, openai, litellm and haystack.                                                                        |
| 0.4.1   | Audit log is stored using GPG. Add bash script (`scripts/gen_and_export_gpg.sh` to prepare GPG keys and simple `scripts/decrypt_auditor_logs.sh` to decrypt encrypted audit logs. Moved core functionality from `base` to module `core` module. Quickstart.                                 |
| 0.4.2   | Fix `first_available_optim` Strategy. Add KeepAliveMonitor to periodically pings model endpoints to keep them warm.                                                                                                                                                                         |
| 0.4.3   | Add custom Prometheus metrices for logging masker/guardrail inidents. Fix OpenAI compatible v1 `/models` endpoint. Introduce monitors: services and keep alive models. Fixed guardrail retunr in case when streaming.                                                                       |
| 0.4.4   | Validate unique provider identifiers. Store all hosts with keep‑alive configured in a Redis. UtilsPlugin pipeline with LangChain based simple RAG plugin (extending context to GenAI with locally built databse). Add handling of `v1/response` endpoint                                    |
| 0.4.5   | Fixed sreaming to LMStudio native. Refactor streaming module.                                                                                                                                                                                                                               |
| 0.4.6   | Added support for embeddings endpoints across all providers. Extended `ApiModel` and `ApiTypesI` with `is_embedding` flag. Added `test_embeddings.py` utility for verifying embedding models through the API.                                                                               |
| 0.4.7   | Integration with native Anthropic API.                                                                                                                                                                                                                                                      |