## Changelog

| Version | Changelog                                                                                                                                                                                                                                                                                   |
|---------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 0.0.1   | Initialization, License, setup, interface for each endpoint and sample `ping` EP. Autoloader of builtin endpoints and for the future implementations.                                                                                                                                       |
| 0.0.2   | Add base models for api call (module `llm_proxy_rest.data_models` with `error.py` handling. Decorators to check required params and to measure the response time.                                                                                                                           |
| 0.0.3   | Proper `AutoLoading` for each found endpoint. Implementation of `ApiTypesDispatcher`, `ApiModelConfig`, `ModelHandler`. Ollama endpoints: `/`, `tags`. Added endpoint to full proxy with params. Streaming in case when external api provides stream.                                       |
| 0.0.4   | All llama-service endpoints are refactored to `llm-proxy-api`. Refactoring base `ep_run` method. Proper handling system message, prompt name, model etc.                                                                                                                                    |
| 0.1.0   | Repository name changed from `llm-proxy-api` to `llm-router`. Added class `HttpRequestExecutor` to handle http requests from `EndpointWithHttpRequestI`. Handled routing between any models: `openai -> ollama` and `ollama -> openai`                                                      |
| 0.1.1   | Prometheus metrics logging. Workers/Threads/Workers class is able to set by environments. Streaming fixes. Multi-providers for single model with default-balanced strategy.                                                                                                                 |
| 0.2.0   | Add balancing strategies: `balanced`, `weighted`, `dynamic_weighted` and `first_available` which works for streaming and non streaming requests. Included Prometheus metrics logging via `/metrics` endpoint. First stage of `llm_router_lib` library, to simply usage of `llm-router-api`. |
| 0.2.1   | Fix stream: OpenAI->Ollama, Ollama->OpenAI. Add Redis caching of availability of model providers (when using `first_available` strategy). Add `llm_router_web` module with simple flask-based frontend to manage llm-router config files.                                                   |
| 0.2.2   | Update dockerfile and requirements. Fix routing with vLLM.                                                                                                                                                                                                                                  |
| 0.2.3   | New web configurator: Handling projects, configs for each user separately.                                                                                                                                                                                                                  | 